import torch
import torch.nn.functional as F
import numpy as np
from gpt2_tokenizer_wrapper import GPT2TokenizerWrapper
from model.gpt_model import MiniGPT
from config import (
    vocab_size, embed_dim, max_seq_len, num_heads, ff_dim, num_layers,
    tokenizer_path, checkpoint_path
)
import datetime
import os
from collections import Counter

# Prepare log file
timestamp = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
log_file_path = f"generation_{timestamp}.log"
log_lines = []

def log(msg):
    print(msg)
    log_lines.append(msg)

# Load tokenizer
tokenizer = GPT2TokenizerWrapper()

# Initialize model and load checkpoint
model = MiniGPT(vocab_size, embed_dim, max_seq_len, num_heads, ff_dim, num_layers)
model.load_state_dict(torch.load(checkpoint_path, map_location="cpu"))
model.eval()

# ğŸ§© Print top-k logits (before softmax)
def print_top_k_logits(logits, k=5):
    topk_vals, topk_idx = torch.topk(logits, k)
    log("ğŸ“Š Raw Logits (Top-5):")
    for i in range(k):
        tid = topk_idx[i].item()
        val = topk_vals[i].item()
        decoded = tokenizer.decode([tid])
        log(f"  {i+1}. Token ID: {tid} â†’ '{decoded}' | Logit: {val:.4f}")

# ğŸ” Print top-k softmax tokens
def print_top_k_tokens(logits, k=5):
    probs = F.softmax(logits, dim=-1)
    topk_vals, topk_idx = torch.topk(probs, k)
    log("ğŸ” Top-k Softmax Probabilities:")
    for i in range(k):
        tid = topk_idx[i].item()
        token = tokenizer.decode([tid])
        prob = topk_vals[i].item()
        log(f"  {i+1}. Token ID: {tid} â†’ '{token}' (prob={prob:.4f})")

# ğŸ¨ Render attention matrix as text heatmap
def render_attention_heatmap(attn_weights, tokens):
    log("ğŸ§² Attention Heatmap [Head 0]:")
    attn = attn_weights[0, 0]  # [seq_len, seq_len]
    attn = attn.detach().cpu().numpy()
    tokens = [t.replace('\n', ' ') for t in tokens]
    header = "     " + " ".join([f"{t:>5}" for t in tokens])
    log(header)
    for i, row in enumerate(attn):
        row_vals = " ".join([f"{w:.2f}" for w in row])
        log(f"{tokens[i]:>4} {row_vals}")

# ğŸ” Token selection logic
def sample_next_token(logits, temperature=1.0, top_k=None, top_p=None, token_ids_so_far=None, repetition_penalty=1.0):
    # Apply repetition penalty
    if repetition_penalty != 1.0 and token_ids_so_far:
        token_counts = Counter(token_ids_so_far)
        for token_id, count in token_counts.items():
            logits[token_id] /= (repetition_penalty ** count)

    # Apply temperature
    logits = logits / temperature

    # Apply top-p sampling
    if top_p is not None:
        sorted_logits, sorted_indices = torch.sort(logits, descending=True)
        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)
        sorted_indices_to_remove = cumulative_probs > top_p
        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
        sorted_indices_to_remove[..., 0] = False
        logits[sorted_indices[sorted_indices_to_remove]] = -float("Inf")

    # Apply top-k sampling
    if top_k is not None:
        topk_vals, topk_idx = torch.topk(logits, top_k)
        logits = torch.full_like(logits, float('-inf'))
        logits[topk_idx] = topk_vals

    # Final token selection
    probs = F.softmax(logits, dim=-1)
    return torch.multinomial(probs, num_samples=1).item()

# ğŸ” Generation loop with full logs
@torch.no_grad()
def generate(prompt, max_new_tokens=30, temperature=1.0, top_k=None, top_p=None, repetition_penalty=1.0):
    input_ids = tokenizer.encode(prompt)
    log(f"\nğŸ“¥ Input Prompt: {prompt}")
    log(f"ğŸ”¢ Tokenized Input IDs: {input_ids}")
    log(f"ğŸ§© Tokens: {[tokenizer.decode([i]) for i in input_ids]}\n")

    input_ids = torch.tensor([input_ids])
    generated_ids = input_ids[0].tolist()

    for step in range(max_new_tokens):
        if input_ids.shape[1] >= max_seq_len:
            break

        logits, _ = model(input_ids)
        next_token_logits = logits[:, -1, :]

        # ğŸ§¬ Embedding vector (last token)
        if hasattr(model.backbone, 'embedding'):
            embed = model.backbone.embedding(input_ids)
            log("ğŸ§¬ Embedding Vector (last token):")
            log(str(embed[0, -1, :].detach().cpu().numpy()))

        # ğŸ“Š Raw logits
        print_top_k_logits(next_token_logits[0], k=5)

        # ğŸ” Softmax tokens
        print_top_k_tokens(next_token_logits[0], k=5)

        # ğŸ§  Sample next token
        next_token_id = sample_next_token(
            next_token_logits[0],
            temperature=temperature,
            top_k=top_k,
            top_p=top_p,
            token_ids_so_far=generated_ids,
            repetition_penalty=repetition_penalty
        )
        token_str = tokenizer.decode([next_token_id])
        log(f"[{step+1:02d}] ğŸ§  Chosen Token ID: {next_token_id} â†’ '{token_str}'")

        # ğŸ§² Attention
        if hasattr(model.backbone.blocks[-1].attn, 'attn_weights'):
            tokens_so_far = [tokenizer.decode([i]) for i in generated_ids]
            render_attention_heatmap(model.backbone.blocks[-1].attn.attn_weights, tokens_so_far)

        log("â€”" * 40)

        generated_ids.append(next_token_id)
        input_ids = torch.tensor([generated_ids])

    final_output = tokenizer.decode(generated_ids)
    log("\nğŸ“¤ Final Generated Token IDs:")
    log(str(generated_ids))
    log("ğŸ§¾ Final Output Text:")
    log(final_output)

    with open(log_file_path, "w", encoding="utf-8") as f:
        f.write("\n".join(log_lines))
    log(f"\nğŸ“ Full log saved to: {log_file_path}")

    return final_output

# ğŸ” CLI Runner
if __name__ == "__main__":
    print("ğŸ§  MiniGPT Generator â€” Explainable Mode")
    print("-" * 30)

    prompt = input("Enter your prompt: ").strip()
    max_new_tokens = input("How many tokens to generate? [default=30]: ").strip()
    max_new_tokens = int(max_new_tokens) if max_new_tokens else 30

    top_k = input("Top-k sampling (enter number or leave blank): ").strip()
    top_k = int(top_k) if top_k else None

    top_p = input("Top-p (nucleus) sampling, 0.0â€“1.0 (leave blank to disable): ").strip()
    top_p = float(top_p) if top_p else None

    temperature = input("Temperature? [default=1.0]: ").strip()
    temperature = float(temperature) if temperature else 1.0

    repetition_penalty = input("Repetition penalty? [default=1.0]: ").strip()
    repetition_penalty = float(repetition_penalty) if repetition_penalty else 1.0

    print("\nâ³ Generating with full trace...\n")
    output = generate(
        prompt,
        max_new_tokens,
        temperature=temperature,
        top_k=top_k,
        top_p=top_p,
        repetition_penalty=repetition_penalty
    )

    print("\nğŸ“ Final Generated Text:\n")
    print(output)
