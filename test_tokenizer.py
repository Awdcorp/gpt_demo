# test_tokenizer.py
from tokenizers import Tokenizer

# Load the trained tokenizer
tokenizer = Tokenizer.from_file("tokenizer/tokenizer.json")

# Try encoding
text = "Hello, how are you?"
encoded = tokenizer.encode(text)
print("ğŸ”  Input text:", text)
print("ğŸ”¢ Token IDs:", encoded.ids)
print("ğŸ§© Tokens:", encoded.tokens)

# Try decoding
decoded = tokenizer.decode(encoded.ids)
print("ğŸ“ Decoded back:", decoded)
